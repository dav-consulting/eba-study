# Executive summary

The amount of digital information in todays world has increased to unimaginable proportions, and the volume is expected to triple just over the next five years. Many expect that this development will continue, pick up pace, and proceed along an exponential growth path into the foreseeable future. Cost reductions for computational power, increasing internet connectivity and society-wide digitalization are examples of drivers behind this seemingly ever-increasing generation of digital content.

This development thus creates an opportunity for governments, organizations and companies to process this information in order to gain knowledge and new insights from it. To take full advantage of this information however, requires both adoption of new techniques relying on computational or machine-based approaches with quick processing, as well as individuals with a skillset that allows them to efficiently understand how to harness these techniques and methods. This has over the last couple of decades led to the realization of a new profession often referred to as *Data science*, which captures this broad skillset involved in handling, processing, analyzing and visualizing large quantities of data. Often this also includes understanding, communicating and applying state-of-the-art methods from computer science, machine learning and artificial intelligence. Furthermore, since much of the available data comes in the form of unstructured text, data scientists also need to make use of the advancements in methods for language understanding and processing known as computational linguistics or Natural Language Processing. These methods are particularly important since they provide the foundations for gaining novel insights from large quantities of unstructured texts in ways that previously were not possible. 

This development has had huge impacts on large parts of society, with applications ranging from classification and organization of texts to machine-based language translation and chatbots. It has also effected how research can be conducted and what insights can be drawn from it. The practices of evaluation is no exception to this, not the least due to its heavy reliance on data to conduct analysis and retrieve insights. Recently, calls have been raised relating to the necessity of a broadened analytical toolbox for evaluations if increases in volume, velocity and variety of data are to be handled and taken full advantage of. This is believed to be the case, particularly within the field of international development cooperation, where evaluations are an important instrument for retrieving insights as well as fostering accountability and sound governance. 

The purpose of this study is to explore the potential of data science and NLP methods, and assess how these methods may be applied to derive meta or secondary data from readily available evaluations within the field of international development cooperation. This entails the development of custom strategies for data handling and processing in order to derive results that replicate traditional evaluation methods relying on manual labor. 

In order to make progress, we make use of a manually annotated or so-called labeled dataset that can guide the machine-base methods that are developed and tested. The labeled dataset comes from a meta study covering the content and conclusions from 128 decentralized evaluations commissioned by Sida between 2012-2014. These evaluations form a central instrument in the follow-up of Swedish projects and programmes within the realm of international development cooperation. The questions in the labeled dataset are directed to certain aspects of the decentralized evaluations with bearing on aspects areas such as geography, funding, thematic area, project sustainability etc. A final component of this study relates to a test of a scaled-up exercise, where the developed methods deemed successful are tested in an automated analytical process. All decentralized evaluations between 2012-2020 (>300) are fetched from Sida's web based archive and automatically analyzed in real-time over the course of a few minutes.  

The initial expectations of this study, were that if successful these methods could be readily applied to derive insights into what past evaluations have concluded regarding a number of questions of relevance for steering future aid projects and programmes. The advantage, in comparison to a manual assessments, would be that apart from generating interesting descriptive statistics and a quick analytical turn around, these methods would also give us better information on the reliability of the statistics produced. By this we mean that since these types of methods are computational in nature, they are also typically more stable in their predictions and errors over time than human beings, which may vary more unpredictably in their assessments depending on a range of factors such as time of day, overall mood or hours of sleep. And since both humans and computer algorithms are likely to produce errors at one stage or another, having an understanding of the size and type of error is thus important. Another advantage with the machine-based approach is thus the possibility to correct for errors. If an error were to be detected in a product from a manual labour process it would require lots of resource to redo the work, while with a machine-based approach the underlying code could be adjusted for the detected error and the analytical process could then be quickly repeated with updated results in a matter of minutes in many cases.

From our results, we have concluded that much potential exists when it comes to designing computational approaches that can derive valuable insights and descriptive statistics from past evaluations in a quick, efficient and reliable manner with a reasonable rate of error. Valuable insights can be generated in ways where scale, scope and timeliness are no longer factors of concern allowing for automation of many repetitive and straining tasks. A majority of the designed strategies/methods in this study have performed relatively well and close to all have performed beyond initial expectations. This does not, however, imply that a machine-based approach and the tested methods are without flaws. A few questions proved to be harder and more complex than initially thought, mainly due to a highly complex language context and limitations in the quality and availability of training data. In these cases more work and higher quality data would be needed to produce desirable results. It should also be mentioned that it is the evaluators language per se that is analyzed in this applied science study. In particular, this is not to be confused with an assessment or appraisal of the performance of the actual projects/programmes that have been subject to the conducted evaluations.

In addition, this study has also revealed discrepancies among human-based assessments. These discrepancies were revealed using a random sample from the labeled dataset, to test whether a third-party expert evaluator would assign the same label as in the original dataset. The purpose of this was to test whether the questions where phrased in a way such that the answer could be easily distinguishable by a third party with good domain knowledge. This validation exercise revealed that the third-party and original assessment, in general did not agree on the appropriate labels to much greater extent than than the machine-based approach did with the original assessment. In particular, this was the case for tasks which were complex in nature and thus required careful judgement on behalf of the evaluator, but it also occurred for more simple questions indicating that the task of conducting a large number of manual assessments may indeed be straining to humans. From these results, we conclude that particular care is needed when phrasing questions in these types of meta studies in order to avoid annotator disagreement due to indistinct or complex labelling that blurs interpretability.

By and large, we believe this study has revealed the there is indeed a potential for data science and machine-based approaches for compiling descriptive statistics for meta-evaluations. However, the value of this approach depends largely on the standards to which we want it to adhere, or in other words what degree of error we are willing to accept. From our perspective there are no straightforward answers to these questions. Answers are likely to vary with the type of decisions that are expected to be taken based on the estimations. Some decisions are likely to require a high degree of accuracy, while others perhaps can settle for less.


<!--

The developed methods in this study are by no means intended to be exhaustive. The study should thus not be viewed as a review of the field of NLP, but rather as a case-based study where various machine-based methods with the ability to scale, have been tested and applied to questions of relevance to international development cooperation. It should also be mentioned that it is the evaluators language per se that is analysed in this applied science study. In particular, this is not to be confused with an assessment or appraisal of the performance of the actual projects/programmemes that have been subject for the conducted evaluations.
-->