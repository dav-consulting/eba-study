# Discussion

A central component of this study has been to observe and take note of strengths and weaknesses in data science and NLP methods when it comes to the challenges involved with the extraction of meta data from evaluations. This chapter discusses these challenges in more detail with particular focus on comparing the performance of human and machine-based approaches to data processing. In order to better understand the results of this study, and to set them in perspective, we also focus on initial expectations one might or can have with regards to a machine-based approach of this sort. At the end of the section more detailed limitations of this study are be spelled out. 

## General strengths and weaknesses with a machine-based approach

There is no shortage of vivid examples where machine-based approaches are outperforming humans[^d1] in close to any field of operations, and this has turned up expectations on the future capabilities of computers and the utilities that it can bring (see e.g. Brynjolfsson et al. 2018). The fact that machine-based approaches are on the rise is difficult to argue against. However, and regardless of the recent advances, the future is always difficult to predict. Early pioneers such as John McCarthy promised as early as the 1960 that general intelligent machines were within grasp (Marr 2017).  Statements like this tend to leave intrinsic details out (or are lifted out of context). The fact of the matter is that there are many factors that need to come together for a computer to beat a human in even the simplest of task, let alone when it comes to human language as in the case of this study. There are simply no shortcuts for how to make ends meet to truly *understand* human language. 

[^d1]: Two well-known examples is IBM's question-answering software Watson that outperformed human-level performance in answering questions posed in natural language in 2011 (IBM 2020), and the other more recent example is Google's Deepmind AlphaGo that was able to take another step and beat humans in a game considered to be intrinsically difficult for computers to master (Deepmind 2019). 

In this light, what is the current state of affairs when it comes to the undisputed advantages in applying a machine-based approach to language understanding? There are a few aspects that cannot be overlooked. The most obvious being those of speed, consistency, and endurance. Speed simply relates to the time it takes to complete an assigned task. The computer operates at blazing speeds for the most part and will thus always outperform humans on repetitive simple tasks. Consistency on the other hand, entails the prowess not to deviate from an assigned task, which many times is a difficult issue for humans. The last general advantage -  endurance - relates to the fact that a computer never tires. In the context of this study these traits are key factors that underlies the potential embedded in a machine-based approach. 

Among these traits, consistency is perhaps the most important one. Apart from the fact that our third party comparison, which shows variation in responses among experts, this phenomenon has also been recognized by other studies revealing inconsistencies both over time and between individuals as an inherently human feature.[^d2] The speed in both collecting and analyzing data are orders of magnitude faster (once up and running), implying that the number of evaluations processed in this study has thus not really been an issue. Advantages with bearing on endurance has similarly not played a major role in this study, since the volume of evaluations is relatively low, and the pace of incoming evaluations is also modest. However, for processes with larger volumes of data streams and where timeliness for delivery is important these traits are of essence. 

[^d2]: The differences or discrepancies obtained from repeating a survey or by replicating some questions in a survey has received much attention (Biemer, 2004). Several studies have highlighted shifts in preferences depending on how the same problem is framed (Tversky and Kahneman, 1981). Inconsistencies of choice has also been observed and model in the brain (Kurtz et.al 2019).

What about the general weaknesses of the machine-based approach involving computational techniques such as NLP? Well, for starters a typical machine-based approach can be seen as a competent expert over a very narrow field. As soon as the context is altered these kinds of systems are potentially in trouble (Alcorn et al 2019). Another central weakness is that of a lack of contextual understanding. Humans typically and by default tend to use preconceived notions with regards to their surrounding as well as the problems that need to be solved. This has proven to be a highly successful strategy for solving problems of many different sorts. And this general flexibility or creativity has been proven to be very difficult to mimic with a machine-based approach. What might seem as the most trivial task, which humans take for granted such as body motor skills, is often an extremely complex task for a computer (see e.g. Minsky 1986 or Moravec 1988). 

For many of the developed strategies in this study this has proven to be at the very center of what has been challenging. The variation in the used language in evaluations is very vivid and as with most NLP tasks it cannot easily be fitted into a rules-based approach. Further, the amount of training data has not been nearly enough to be able to effectively train statistical models for every task. However, even if more data had been available the task would still have been challenging due to the discrepancies in how humans interpret language. In particular, this is something that was revealed in our third party validation assessment, which showed that the answers to the several questions in the analytical framework were not always aligned between our independent third party expert and the LME-dataset. These factors are clearly important to consider when training machines to complete these tasks, since any errors or biases in the data will of course also be reflected in the model utilizing this data in its training process (Mitchell 2019). 

Another aspect which is worth considering when evaluating machine-based approaches are the energy consumption that goes into training these models. For example, in a widely cited study by Strubell et.al. (2019) it was estimated that training a single deep learning model like the BERT or GPT-2 model can generate CO2 emissions corresponding to about single passenger flight between New York and San Francisco. Given that these types of models are actively being developed and continuously growing in size it has thus becomes increasingly important that they are also made publicly available via open source in order to avoid unnecessary carbon dioxide emissions from model training.[^d2a]

A final important aspect, is the question when the methods developed and tested in this study, and NLP in general, are preferable as an approach for drawing inference regarding a large corpus of evaluations as opposed to drawing inference based on a smaller sample assessed by a contextual expert. This is a difficult question. First, it is connected to the statistical discourse on the appropriate sample size needed to avoid detecting false effects when there is none (often referred to as Type I errors) but also reducing the probability of not detecting an effect when one exists (Type II errors). For example, if we wish to assess on average how often evaluation studies conclude that the evaluated project/projects are deemed sustainable then the question is how many studies do we need to assess in order to feel confident that the sample is representative of the full population of studies. The answer to this question depends on several factors such as, the extent to which we are willing to accept the occurrence of Type I and II errors, as well as how far the actual population mean deviates from our ex-ante hypothesis of the correct mean. Second, the question also relates to the finding in this and several other studies that inter-annotator agreements can be low even among experts. In particular, we have found that when questions are not well defined and when answers to questions (labels) tend to overlap this creates problems both for assessments made by contextual experts as well as for machine-based algorithms, which is also inline with other studies showing that the precision of the results relies heavily on the way the question has been phrased and how distinct the answers tend to be (Artstein, 2017).  Finally, the verdict on when to use machine based approach versus contextual expertise comes down to a question of organizational resources and requirements, in particular issues such as transparency of methods, replicability and scalability of the assessment. If an organization, aims to repeat an assessment at regular intervals, machine-based approaches are very advantageous while if an assessment is a one shot exercise it depends more on requirements such as transparency as well as the weighing in the issues raised above. In economic jargon, one could say that machine-based approaches typically are associated with high fixed costs but low running costs while human-based approaches involve a low fixed costs but with high running costs.


## Observed limitations in the study

<!--
The answer to this question thus depends on how sure we need to be that the metric calculated in our sample is representative of the whole population of evaluations. 
Finally, given these aspects and the results of this study a practioner might ask the question as to when the
Given the purpose of this study it is crucial that observed limitations in the study and the developed strategies are thoroughly discussed. This is important of several reasons, and the chief among them, being that reader should understand what the study entail and what it does not. 
-->

Before getting into the limitations, it is first and foremost, important to emphasize that it is the language in the processed evaluations that has been analyzed, which is not to be confused with the data reflecting the performance of actual evaluated projects/programmes per se. In other words, this study reviews the evaluator's language and their, sometimes subjective, assessments of the projects/programmes performance. This entails that personal writing styles may (or may not) affect the outcome of the results in this study. And as we have learned (from the results section) there are a relatively large number of individuals involved in the processed evaluations. In aggregation, this means that the context that has been analyzed is relatively stochastic and the outputs from the designed strategies should be viewed as more or less qualified estimations and thus not to be confused with absolute certainties. Given this context we believe the following limitations are particularly worth discussing.

Firstly, after the initiation of the study we detected limitations in the quality of the data which was not known to us at the outset. This realization came as a result of reviewing parts of the LME-dataset, which made us realize that this dataset is not guaranteed to be fully accurate in terms of reflecting a unanimous agreement on the correct answer to all the questions posed. This in turn created a challenge for both training and testing of the strategies developed in this study. This is particularly true for a few of the the more complex questions dealing with, the in many cases, difficult judgments in the evaluations (e.g. judgement whether or not the project/programme is deemed sustainable, and what thematic field a project/programme should be sorted under). It should, however, be emphasized that this is not an uncommon issue and tends to arise quite frequently when similar exercises are attempted at scale (see e.g. Kurtz et.al. 2019). In particular, human conducted annotations for statistic models and machine learning has proven not to be without flaws. Machine learning practitioners have observed contradicting labelling decisions in human performance, sometimes by the same person over the course of a single day. This has raised questions to the robustness of accuracy tests that relies heavily on [Human-Level Performance (HPL)](#Other) (Ng 2020). It is in other words hard to secure flawless training- and test data. The uncertainty this brings has thus clearly affected this study and there is thus an unknown and inherent margin of error that effects both training and testing. 


Second, this study has not conducted any normative-based assessment of the questions or their relevance for the field of international development cooperation. The questions have rather been a benchmark for assessing and testing the developed strategies, and hence served as a guide for assessing the potential in data science and NLP methods for producing meta evaluations. Many of the selected questions from the LME-dataset settles for checking for content rather than estimating insights. For instance, several questions focus on aspects such as if a concept is discussed in the evaluations, rather than what is concluded about the same concept. 

Third, the formulation of the questions and the predetermined response categories have in many cases been challenging. For example, the sixteen various thematic response labels that are included in question 11, which in several cases are semantically similar, makes it a difficult NLP task to separate one from the other with [word embedding](#method) that are used in this study. There is also a certain degree of inconsistency in the available labels in LME-dataset, which have brought limitations for how some of the questions could be answered. 

A final and central limitation in this study is tied to the scope. All questions, without exception, would benefit from additional adjustments and the accuracy would with little doubt increase if more time could be spent on fine tuning the developed strategies. In hindsight, it might have been more beneficial for showcasing and exploring the potential in data science and NLP if the number of questions had been reduced and thus allowed for more time to be spent on a smaller subset of the selected questions. This could for example have allowed for proper context bound [Natural language annotation](#other)s to be incorporated in the study and increased the volume of training data for the more complex questions. 


## Moving forward

 A nation wide assessment of the current state of affairs for using AI techniques in Sweden stipulated that there is both potential and a pressing need to automate existing operations in order to improved organizational efficiency and data reliability in many organizations (Vinnova, 2018).[^d3] In fact, the ability to harness these techniques is stressed to be important for any organization's future competitiveness. Inline with these suggestions this study has tried to explore techniques that can be used by organizations to take a step in this direction.

Apart from the results reported and discussed above, several additional research questions have come to light, which we think would be of general interest for future studies. In several cases these questions should be of particular interest for Sida as areas to follow-up on for how to process and assess their decentralized evaluations.

 - Explore ways for how to combine advantages from a machine-based approach with the strengths of a manual labour approach. Both approaches have clear advantages, often for different types of tasks, and we think it would be valuable to research how a mixed-approach could be setup and assess how this can bring explicit value to an organization.  This study and the developed strategies, such as the generation meta data from unstructured narrative texts could help facilitate manual assessments in terms of both speed, accuracy and consistency. For instance, the designed algorithm that extracts text passages with a certain content can dramatically reduce the workload of a manual process, and thus establish a good balance where the advantages from a machine-based approach is joined with the strengths from a manual validation assessment. Approaches like this are sometimes referred to as intelligence augmentation, and the most common and widely used example is online search engines, which bring vast improvements to manual work processes (see for example Jordan 2019). In the case of this study, and the stated example of extracting relevant content, a dashboard could preferably be developed where data obtained by a machine-based approach is made available for additional and final manual scrutiny and/or usage.

 - Take advantage of generated meta data and explore possibilities to merge it with other existing data sets. For example, make cross-references between evaluation frequency of countries and the amount of ODA volumes - is there a correlation between funding and follow-up? This could be viewed in a commonly accepted perspective where 1% of the contribution funds should be used for evaluation. How close or far off from this is the current operations? Are there regional variations? It should be noted that we, in this study, initially pursued possibilities to find ways to connect the processed evaluations with contribution statistics. This would for instance be relevant for question 12 in the LME-dataset. Based on discussion with Sida staff from the evaluation unit the majority of the decentralized evaluations were however not tied to Sida's contribution statistics, and it was concluded that there was no easy way to connect the two datasets. This could be worth pursuing, particular since it would likely bring much value for policy makers. It is also possible that this could be used to train more accurate statistic-based models since the labels, for at least some variables, are deemed to be unquestionable in the contribution statistics. 
 
 - The overrepresentation of Eastern African countries among the processed evaluations can serve as an interesting follow-up to assess reasons for this pattern. Possible questions may be: How does this result compare the amount of ODA to the respective countries? 

- Several of the developed strategies in this study recorded additional meta data that was not used in the study. This was the case for the developed strategies that extracted data on geographical entities where we also recorded data on OECD/DAC donor countries and donor organizations. This could serve as a point of departure for looking into the possibility to conduct research on how projects/programmes are funded. This data could also be used to give estimations for who Sida and Sweden is collaborating with, and thus feed an analysis or estimations on key objectives in the Paris Declaration on Aid Effectiveness relating to, for instance, donor alignment and harmonization. 
- Contemplate adjusting the formulation of research questions and precoded response alternatives to better suit a machine-based approach. As mentioned in the results section, there are low hanging fruits for improving the accuracy of a machine-based approach, for instance by limiting the number of thematic labels and making sure that selected categories are semantically diverse. There is also a possibility for this kind of approach to be set up to estimate the level of adherence to several thematic areas (i.e. many contributions are probably in the intersect of two of more thematic categories and can thus be considered to be labeled as more than one).
- Last, but not least, consider allocation of resources for [Natural language annotation](#other) and development of a high quality training data set/s for more complex tasks within the field of international development cooperation. This would improve the accuracy of the trained models even for the more complex tasks, such as labelling evaluations based on thematic focus and/or OECD/DAC evaluation criteria




[^d2a]: For example, the current GPT-2 is about one hundred times smaller, compared to its successor the GPT-3 model and therefore requires a substantial amount of more training cycles and energy.



[^d3]: Among 170 targeted Swedish authorities only 6% stated that they at the time had ongoing projects with AI technology (Vinnova 2018).



