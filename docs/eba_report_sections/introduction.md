# Introduction

Recent technological developments have made available vast quantities of digital texts and recordings in a seemingly ever-increasing share of human communication and online interaction. As an example, recent estimates suggest that the total amount of global data will grow from 60 to 175 Zettabytes between 2020 and 2025 (Reinsel et al. 2020). Furthermore, it is also believed that 60% of the worlds population have access to and are using the internet as of 2020 (ITU 2020) and that the global mobile phone usage, based on number of subscriptions, has surpassed the population of the world (ITU 2018). These estimates hint that huge changes are coming and the projections for these trends are that they will continue, and in many cases pick up in pace. The digitalization and amount of information generated in society today is even expected to grow exponentially into the foreseeable future (Kurzweil, 2004).

In order to take advantage and efficiently process this vast amount of information, various computational or machine-based approaches are needed. The development of such approaches has been underway ever since the introduction of computers, and are increasingly being applied in all areas of daily life where it is believed to be capable of freeing up human labour from tedious and repetitive tasks. This has also led to the growth of a new profession often referred to as *Data science* which captures the broad skillset involved in gathering, processing, extracting value, visualizing and communicating information from data (Varian, 2009). Often this involves building complex quantitative algorithms and models to organize and synthesize large amounts of information, a process which typically relies upon state-of-the-art research in computer science such as the field of artificial intelligence ([AI](#other)) or machine learning ([ML](#other)). In particular, since much of this data comes in the form of unstructured text, data scientists also make frequent use of methods developed in the field of computational linguistics or *Natural Language Processing* ([NLP](#other)) to derive meta data for their analysis.

From a social science perspective, this development has also had a big impact. In particular, the information encoded in text has turned out to be a rich complement to the more structured kinds of data traditionally used in research, and recent years has seen an explosion of research in areas such as political science and economics using text as data (Gentzkow et.al., 2019). Likewise, the described development is believed to have a direct effect on the practice of evaluations, not the least due to its heavy reliance on data to conduct analysis and retrieve insights. Here calls have been raised relating to the necessity of a broadened analytical toolbox for evaluations if increases in volume, velocity and variety of data are to be handled and taken full advantage of (see Petersson et al. 2017). This is believed to be the case, particularly within the field of international development cooperation, where evaluations are an important instrument for retrieving insights as well as to foster accountability and sound governance. 

The purpose of the present study is therefore to explore the potential of data science and NLP methods in order to assess how these methods may be applied and used to derive meta data for a systematic review[^i1] of sort for evaluations within the field of international development cooperation. This entails an assessment of the performance of these machine-based approaches in relation to traditional evaluation methods relying on manual labor. More specifically we aim to i) apply a selection of these methods and test how they can be used to derive secondary (meta) data for a systematic review of what past evaluations have concluded about aid projects and programmes; and ii) in the process evaluate what the strengths and weaknesses of these methods may be compared to standard approaches relying on manual labor.

To address these questions the study relies on a wide range of methods with a heavy emphasis on computational linguistics, or so-called NLP. This field has experienced a rapid development during recent years and have as of now, proven to have the ability to reliably perform a variety of labour intensive and analytical tasks ranging from document summarization to text classification at scale. It may thus be expected that these methods could also bring new insights into the field of international development cooperation by assessing what past evaluations have concluded regarding a number of indicators of relevance for steering future aid projects and programmes. The advantage, in comparison to manual assessments, would be that apart from generating interesting descriptive statistics and a quick analytical turn around, these methods would also give us better information on the reliability of the statistics produced. By this we mean that since the methods are computational in nature, they may also typically more stable in their predictions and errors over time than human beings, which may vary more unpredictably in their assessments depending on e.g., time of day, overall mood or hours of sleep (Ng, 2020). More specifically, once a computational methodology has been implemented, a manual assessment of the results can both give us an indication with regards to the size of the margin of error as well as insights into how it might be decreased. Since both humans and computer algorithms are likely to produce errors at one stage or another, having an understanding of the size and type of error is thus important. Another major advantage once a reliable machine-based approach has been developed is that the scale (i.e. the number of evaluations included in the meta-analysis) are of little importance for the overall effort required to conduct the analysis.

In order to make progress on using these methods, we make use of a dataset provided by [EBA](#other) featuring a manually annotated or so-called labeled dataset. The labeled dataset (henceforth [LME-dataset](#other)) comes from a meta study[^i2] covering the content and conclusions from 128 decentralized evaluations commissioned by [Sida](#other) between 2012-2014. These evaluations form a central instrument in the follow-up of Swedish projects and programmes within the realm of international development cooperation. The LME-dataset thus constitutes the part of the mentioned EBA meta study that subjects each of the 128 evaluations to a battery of questions[^i3] that covered areas such as geography, funding, thematic area, project sustainability etc. More specifically, the questions are directed to certain aspects of the decentralized evaluations, which have been deemed as relevant for what to incorporate in this meta-evaluation. The aim is to generate an overarching assessment of what the evaluations include, and to a certain extent what they conclude in an attempt to bring value and inform interested stakeholders and policy makers of progress made or lack thereof. 

Based on the questions in the LME-dataset we have singled out a subset of questions where our initial hypothesis was that a strategy based on data science and NLP methods might have success in terms of its ability to generate reliable answers to the questions in the LME-dataset. The analytical framework encompassing the selected questions (appendix 1.1) thus provides a coherent structure for assessing the potential in a variety of computational approaches that we call question-specific strategies, where the output from each such strategy could be compared against the manual assessments made in the LME-dataset.

The study and the used methods within are, however, by no means intended to be exhaustive, and the study should definitely not be viewed as a review of the field of NLP, but rather as case-based study where various machine-based methods with the ability to scale, have been tested and applied to questions of relevance to international development cooperation. It should also be mentioned that it is the evaluators language per se that is analyzed in this applied science study. In particular, this is not to be confused with an assessment or appraisal of the performance of the actual projects/programmes that have been subject for the conducted evaluations.


The study is structured as follows. Section 3 will elaborate on the study's overall work approach and elaborate on detailed steps as well as explain and define the specific methods and benchmarks used throughout the study. This includes the analytical methods as well as the conventions on how to measure progress and the quality of the conducted work. Section 4 will present the designed question-specific strategies for each selected question as well as show the results/predictions from an exercise where the designed strategies are applied on all available Sida evaluations that stretches between 2012 to 2020. This section is furthermore divided into subsections based on the selected question's thematic focus - Data collection and parsing of documents; Geography and time; Funding and donors; Thematic area; and [OECD/DAC evaluation criteria](#other). Section 5 focuses on observed strengthens and weaknesses in the designed and applied strategies, and is concluded with a broader discussion on future research questions and response to the study’s research questions. The final section holds the concluding remarks drawn from this study. 


[^i1]: This study's use of the term *systematic review* refers to application of computational technics for replication and automation of collection, analysis and synthesis of larger volumes of secondary data. This study's usage of the mentioned term should thus not be confused with systematic reviews that aim to scrutinize aspects such as the quality in the study design, used data sources etc. 

[^i2]: ​Livslängd och livskraft: Vad säger utvärderingar om svenska biståndsinsatsers hållbarhet? (Burman 2017).

[^i3]: This type of labelled dataset is often used in supervised learning (an area of machine learning), where algorithms are fed with predefined answers to questions from which it can learn make informed guesses to previously unseen questions.

<!--

The design has been highly valuable, although not without flaws, and has generated a structure where the potential of a multitude of data science and NLP methods have been put to the test.
In summary, two overarching questions have guided our process: 

    1. How can Data Science methods and Natural Language Processing be used to derive secondary data for a review of what past evaluations have concluded about aid projects and programmes?

    2. What are the strengths and weaknesses of these methods compared to approaches relying on manual techniques?

-->
<!--
The potential as well as the value of receiving insights from alternative evaluation methods - besides the more traditional such as interviews, surveys and focus groups - are believed to increase with the intensification of digitalisation as well as increased social engagement on the internet. The amount of traffic and hence available data from alternative data sources, such as online and mobile usage, have reached close to unimaginable proportions and are expected to grow exponentially in the foreseeable future. Recent estimates suggest that the total amount of global data will grow from 60 to 175 Zettabytes between 2020 and 2025 (Reinsel et al. 2020). It is believed that 60% of the worlds population have access and are using the internet as of 2020 (ITU 2020). The global mobile usage, based on number of subscriptions in the world, has surpassed the population of the world (ITU 2018). The mentioned patterns show that huge changes have happened and the projections for these patterns or trends are that they will continue and in many cases pick up pace.
-->
