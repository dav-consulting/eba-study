
# Concluding remarks 

In this study we have applied data science and NLP methods to extract meta information on specific topics of interest with regards to what past evaluations have conclude about aid projects and programmes. We have found that these techniques indeed can provide useful insights which on several occasions appear to be on par with the errors one might expect from a manual  (human based) assessment of evaluations. 

Apart from accuracy, the advantages of machine-based approaches include speed, consistency and endurance in processing the relevant data. Descriptive statistics can be compiled in a quick, efficient and reliable manner with a reasonable rate of error for most of the tested areas and questions. In short, insights can be generated in ways where scale, scope and timeliness are no longer factors of concern. This further implies that there is scope for automation when it comes deriving interesting insights from conducted evaluation studies.

<!--  The developed strategies and methods in this study outperform manual assessments of Sida's decentralised evaluations many times over when compared against these benchmarks. -->


This does, however, not imply that a machine-based approach and the developed strategies are without flaws. Several, limitations have been observed when applying these methods. Obvious challenges with a machine-based approach lies in its inability to appropriately process outliers. The methods also do not fare well with imperfections in training data and/or with larger deviations in the structure and/or content of the evaluations. We have also concluded that the applied methods require careful tuning to effectively answer the questions in this study which sometimes imply larger costs for getting these processed up and running. In particular, we found some of the selected questions were difficult to tackle with a machine-based approach, mainly due to the complexity of the language in the evaluations. Generally these methods work best when the language is distinct and clear, but struggle when processed texts contain many subtle statements and/or ironies (which however also holds true for humans). 

<!-- 
In particular, twell for repetitive tasks such as assessing the geographical coverage and time periods of evaluations. 

In this light the analytical framework and selected questions from EBA2017:12 surved its purpose to put chosen and applied strategies to the test. However, the analytical structure was less optimal in its attempt to showcase the highest possible performance of machine-based approaches. -->

The study has also revealed discrepancies among human based assessments of this kind. This conclusion was drawn from a manual assessment, by a third-party and independent evaluator, based on a random sample of evaluations which was also part of this study. The results from this validation exercise revealed, inline with our conclusions regarding the machine-based performance, that the third party answers to the questions also deviated in many cases from the originally conducted assessment (LME-dataset). In particular, this was the case for the specific questions which were  more complex in nature and thus required careful judgement on behalf of the annotator, but it also occurred for more simple questions indicating that the task of conducting a large number of manual assessments may be straining to humans. These insights thus calls for particular care when it comes to phrasing questions for these type of meta studies in order to avoid large variations in annotator agreement due to e.g. indistinct or complex labelling which poses a challenge to interpretability. Ideally, a pilot or pre-study test could be conducted that brings in several annotators to ensure questions are phrased in a way such that the answers remain consistent across the annotators. Another potential advantage with a machine-based approach, which can help mitigate situations like this, is the ability to build in checks and balances in order to better understand what type of errors may arise. This also includes the possibility to easily correct for errors discovered at later stages in a process. That is, if an error were to be detected in a product from a manual labour process it would require lots of resource to redo the work, while with a machine-based approach the underlying code could be adjust for the detected error, and the analytical process could then be quickly repeated with updated results in a matter of minutes in many cases.


To summarize, this study  has found that machine based methods have performed quite well for the majority of the questions addressed herein. Importantly, this conclusion is not solely based on the extent that the machine based assessment aligned with the original manual assessment, but also on the fact that the third-party assessment did not align much better with the original assessment than a machine-based approach did for many of the questions. We thus believe there is a potential upside in adopting machine-based approaches for compiling e.g. descriptive statistics for meta-evaluations, but the value depends largely on the standards to which we want the results to adhere, or in other words what degree of error we are willing to accept in our assessments. From our perspective there are no straightforward answers to these questions. Answers are likely to vary with the type of decisions that are expected to be taken based on the estimations. Some decisions are likely to require a high degree of accuracy, while others perhaps can settle for less.

<!-- 

To summarize, in this study we have found that machine based methods have performed quite well for the majority of the questions addressed. Importantly, this conclusion is not based on the extent that the machine based assessment aligned with the original manual assessment but rather on the fact that the third-party assessment did not align much better with the original assessment than a machine based approach did for many of the questions. This however, does not necessarily mean that the machine based approach works well for all types of questions raised in these types of meta evaluations but rather that there is a need to be very carefull when phrasing questions for meta studies in order to avoid large variations in annotator aggreement due to e.g. indistinct or complex labels which challenge interpretabilty. Ideally, a pilot study which brings in several annotaters to ensure questions are phrased in a way that the answers remain consistent across the annotaters.

In this light, the decision to deploy a machine-based approach is unlikely to escape the trade-off between accuracy and readily available estimations.  -->

