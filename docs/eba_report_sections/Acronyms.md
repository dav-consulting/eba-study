# Acronyms and definitions
## Methods
**BERT** - Bidirectional Encoder Representations from Transformers  
**NER** - Named-Entity Recognition is a sub-task of NLP with the aim to identify and classify named entities mentioned in unstructured text into pre-defined categories.  
**Sentiment classifier** - Applies sentiment analysis system for text analysis which combines natural language processing (NLP) and machine learning techniques to classify text as (positive, negative or neutral) based on a computed sentiment scores.  
**Tf-Idf** - term frequency–inverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.  
**Transformers** - A type of deep neural network designed mainly to handle sequential data such as natural language. The transformers technology have paved the way for efficient pre-trained models of language, such as BERT and GPT.  
**Web scraper/scraping** - A process used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler.  
**Word embeddings** - Word embeddings are efficient, dense vector representations of words in which similar words have a similar encoding. They are capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.  
**Word2vec** -  is a family of model architectures and optimisations that can be used to learn word embeddings from large datasets.  
**Zero shot learning** - is the approach when the neural network is forced to make classification for classes it was never trained for.  In other words, the ability to detect classes that the model has never seen during training. It resembles our ability as humans to generalise and identify new things without explicit supervision.  

<!-- Pushp, P.K. and Srivastava, M.M., 2017. Train once, test anywhere: Zero-shot learning for text classification. arXiv preprint arXiv:1712.05972. -->


## Open source packages

Python dependencies and packages that have been be utilised in the study include:


**country-converter** - country converter is a Python package to convert and match country names between different classifications and between different naming versions. (https://pypi.org/project/country-converter/)  
**FuzzyWuzzy** - FuzzyWuzzy string matching package. It uses Levenshtein Distance to calculate the differences between sequences. (https://github.com/seatgeek/fuzzywuzzy)  
**Gensim** - Gensim is an open-source library for unsupervised topic modelling and natural language processing, using modern statistical machine learning. (https://radimrehurek.com/gensim/)  
**Huggingface's transformers** - State-of-the-art Natural Language Processing library which provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, question answering, summarisation, translation, text generation, etc in 100+ languages. (https://huggingface.co/)  
**JSON** - JavaScript Object Notation is an open standard language-independent data format, that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and array data types (or any other serialisable value). (https://en.wikipedia.org/wiki/JSON)
**Jupyter** - Project Jupyter is a nonprofit organisation created to "develop open-source software, open-standards, and services for interactive computing across dozens of programming languages". (https://jupyter.org/)  
**NLTK** -  NLTK is a leading platform for building Python programs to work with human language data. (https://www.nltk.org/)  
**Numpy** - NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. (https://numpy.org/)  
**PyMuPDF** - A package for reading pdf files. The package opens pdf documents page per page and saves all its content in a block and identifies the text size, font, colour and flags. (https://pypi.org/project/PyMuPDF/)  
**Scikit-learn** - Scikit-learn is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms and more. (https://scikit-learn.org/stable/)  
**Scrapy** - Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing. (https://scrapy.org/)  
**spaCy** - spaCy is a library for advanced Natural Language Processing in Python and Cython.  (https://spacy.io/)    
**Streamlit** - Streamlit is an open-source app framework for Machine Learning and Data Science teams.(https://www.streamlit.io/)   
**SQLite** - SQLite is a relational database management system contained in a C library. In contrast to many other database management systems, SQLite is not a client–server database engine. (https://www.sqlite.org/index.html)   
**Pandas** - Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool built on top of the Python programming language. (https://pandas.pydata.org/)   
**PDFminer** - PDFMiner is a text extraction tool for PDF documents. (https://github.com/euske/pdfminer)   
**Plotly** - Plotly creates & stewards the leading data vizualisation & UI tools for machine learning, data science, engineering, and sciences. (https://plotly.com/)   
**pycountry** - pycountry provides the ISO databases for the standards relating to languages, countries, deleted countries, subdivisions of countries, currencies and scripts. (https://pypi.org/project/pycountry/)  
**PyMuPDF** - MuPDF can access files in PDF, XPS, OpenXPS, epub, comic and fiction book formats, and it is known for both, its top performance and high rendering quality. (https://pypi.org/project/PyMuPDF/)  
**Python** - Python is an interpreted, high-level and general-purpose programming language. (https://www.python.org/)  

## Other

**AI** - Artificial Intelligence encompass a wide range of subfields, and even though there is no universal definition, a common feature is the design of intelligent systems, which stretches in a continuum from simple task such as regulating indoor temperature to designing general intelligence in line with human-level intelligence (Russel and Norvig 2016).  
**Data Science** - a discipline in the intersection of mathematics, statistics and computer science where data is the underlying driver to analysis and for retrieving insights.  
**EBA** - The Expert Group for Aid Studies.  
**HLP** - Human-Level Performance is a benchmark technique where the machine learning models accuracy is compared to human performance on a given task.  
**LME-dataset** - Labeled Meta-Evaluation Dataset from the study EBA2017:12 that has enabled this study with an analytical framework used as a training- and test dataset.  
**Machine-based approach** - Refers to an automated approach that is made possible by the capacity of computers, computer programming and storage capacities.  
**ML** - Machine learning refers to processes where computer algorithms have been crafted to learn and make predictions based on previous observations.  
**NLP** - Natural Language Processing.
**Natural language annotation** - Refers to the process of establishing meta data or descriptive lables to underlying observations (or any kind of data) with the purpose to augment an algorithm's capability to give accurate predictions (Pustejovsky and Stubbs 2012). 
**ODA** - Official Development Assistance.  
**Open Source Packages** - Publicly available libraries containing plug and play code. There are currently over 137000 Python libraries with a vast variety of use-cases and functionality that can be accessed and utilised free of charge. For additional information see (https://www.pypi.org).  
**OECD/DAC** - Organisation for Economic Co-operation Development and the Development Assistance Committee.  
**OECD/DAC evaluation criteria** - OECD/DAC evaluation criteria are a set of standards for how to structure and design an evaluation within the field of international development cooperation. for details see https://www.oecd.org/dac/evaluation/daccriteriaforevaluatingdevelopmentassistance.htm  
**Project repository** - The complete data base of this study. This includes the full source code as well as all written excerpts in this report as well as the developed web-based dashboard with descriptive statistics from the study results.  
**QSS** - Designed strategies composed of various data science techniques and natural language processing methods to establish automated and robust machine-based approaches for responding to the selected questions from the LME-dataset.  
**Sida** - The Swedish International Development Cooperation Agency.  